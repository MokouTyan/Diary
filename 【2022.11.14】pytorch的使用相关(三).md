【2022.11.14】pytorch的使用相关(三)

### 参考资料

[ShusenTang/Dive-into-DL-PyTorch: 本项目将《动手学深度学习》(Dive into Deep Learning)原书中的MXNet实现改为PyTorch实现。 (github.com)](https://github.com/ShusenTang/Dive-into-DL-PyTorch)

### .requires_grad

`Tensor`是这个包的核心类，如果将其属性`.requires_grad`设置为`True`，它将开始追踪(track)在其上的所有操作（这样就可以利用链式法则进行梯度传播了）。完成计算后，可以调用`.backward()`来完成所有梯度计算。此`Tensor`的梯度将累积到`.grad`属性中。

`y.backward()`时，如果`y`是标量，则不需要为`backward()`传入任何参数；否则，需要传入一个与`y`同形的`Tensor`

### grad_fn .is_leaf

每个`Tensor`都有一个`.grad_fn`属性，该属性即创建该`Tensor`的`Function`, 就是说该`Tensor`是不是通过某些运算得到的，若是，则`grad_fn`返回一个与这些运算相关的对象，否则是None。

直接创建的，所以它没有`grad_fn`, 而y是通过一个加法操作创建的，所以它有一个为`<AddBackward>`的`grad_fn`。

直接创建的称为叶子节点，叶子节点对应的`grad_fn`是`None`。

```python
import torch

x = torch.ones(2, 2, requires_grad=True)
print(x)
print(x.grad_fn) # None

y = x + 2
print(y)
print(y.grad_fn) # <AddBackward0 object at 0x7f22f7b5a650>

print(x.is_leaf, y.is_leaf) # True False

z = y * y * 3
out = z.mean() # Tensor元素平均值
print(z)
print(out) # tensor(27., grad_fn=<MeanBackward0>)
```

### 求梯度

这里是out`关于`x的梯度

![image-20221114140452046](https://i0.hdslb.com/bfs/album/65cce4e94c05f7c2a6f8f4e1046cf9e77f30c68f.png)

这里求导是直接求到X步骤的

out=z的元素之和求平均值，1/4

z=3y^2

y=x+3

然后将整个带x的式子求导

![image-20221114140552396](https://i0.hdslb.com/bfs/album/ddeb20a00ab2879bb6109daba1086112556e5baa.png)

换个x输入再试一次

![image-20221114141433789](https://i0.hdslb.com/bfs/album/0766b36235e4d8e92104a47f0e2fc1c14ddacf18.png)

### grad.data.zero_()

grad在反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把梯度清零。

![image-20221114143547355](https://i0.hdslb.com/bfs/album/5310d87f13762f9bb9fa8e7dfe4d9baf19e6fb48.png)

因为sum的求导为1

所以看到可以看到由4.5到5.5到6.5

![image-20221114144428627](https://i0.hdslb.com/bfs/album/894fde1bef5b55d4c18d83bfd5ccd4f9f4981bab.png)

### 只允许标量对张量求导

 简单来说就是为了避免向量（甚至更高维张量）对张量求导，而转换成标量对张量求导。举个例子，假设形状为 `m x n` 的矩阵 X 经过运算得到了 `p x q` 的矩阵 Y，Y 又经过运算得到了 `s x t` 的矩阵 Z。那么按照前面讲的规则，dZ/dY 应该是一个 `s x t x p x q` 四维张量，dY/dX 是一个 `p x q x m x n`的四维张量。问题来了，怎样反向传播？怎样将两个四维张量相乘？？？这要怎么乘？？？就算能解决两个四维张量怎么乘的问题，四维和三维的张量又怎么乘？导数的导数又怎么求，这一连串的问题，感觉要疯掉…… 为了避免这个问题，我们**不允许张量对张量求导，只允许标量对张量求导，求导结果是和自变量同形的张量**。所以必要时我们要把张量通过将所有张量的元素加权求和的方式转换为标量，举个例子，假设`y`由自变量`x`计算而来，`w`是和`y`同形的张量，则`y.backward(w)`的含义是：先计算`l = torch.sum(y * w)`，则`l`是个标量，然后求`l`对自变量`x`的导数。

[PyTorch 的 backward 为什么有一个 grad_variables 参数？ - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/29923090)

![image-20221114151041981](https://i0.hdslb.com/bfs/album/7d348642cb4e765ef94f1e604553c2f190d20091.png)

修改参数后，可以得到计算的公式

![image-20221114152021430](https://i0.hdslb.com/bfs/album/c6b295b71e9d903c18bd443956e5a87bd7c76f5d.png)

z是从y得来的，y是x^2，求导后是2x

但是因为z 不是一个标量，所以在调用backward时需要传入一个和z同形的权重向量进行加权求和得到一个标量

### 中断梯度追踪

![image-20221114155019454](https://i0.hdslb.com/bfs/album/1bfbb3155c87501554220dc685c228eafd5ccf04.png)

可以看到，上面的`y2`是没有`grad_fn`而且`y2.requires_grad=False`的，而`y3`是有`grad_fn`的。如果我们将`y3`对`x`求梯度的话会是多少呢？

![image-20221114155653765](https://i0.hdslb.com/bfs/album/0728f29354d1e52dc2d09dd038021e993a193710.png)

为什么是2呢？见上图，由于 y2定义是被`torch.no_grad():`包裹的，所以与 y2 有关的梯度是不会回传的

### 修改data

如果我们想要修改`tensor`的数值，但是又不希望被`autograd`记录（即不会影响反向传播），那么我么可以对`tensor.data`进行操作。

![image-20221114165325267](https://i0.hdslb.com/bfs/album/e22781d2c17c03ad1d5c1b6aee27ac45d63d9ea8.png)