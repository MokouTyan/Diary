【2022.11.01】机器学习记录（From李宏毅）

学习来源均来自：https://www.bilibili.com/video/BV1Wv411h7kN

![image-20221101094633217](https://i0.hdslb.com/bfs/album/704943f4f8e13892986e91a179bdc22197dd63ee.png)

当我们设定函数的时候，不一定是线性的情况下，可以将一个曲线图拆分成多个函数相加

![image-20221101143518565](https://i0.hdslb.com/bfs/album/b2b7e62d3b60d4cab3e28451815300673c56d24a.png)

正如下图中的红色图，是由四条曲线相加得到

![image-20221101143453377](https://i0.hdslb.com/bfs/album/f6fbc4b41ea22267cef24fec0d19361938102640.png)

如果要将折线图模拟成曲线图的话，一般使用sigmoid来逼近原来的折线图

![image-20221101143427021](https://i0.hdslb.com/bfs/album/ce518ab101e05b68076bca1b5523ce04fb8fd138.png)

改变不同的参数获得的不同结果

![image-20221101144015706](https://i0.hdslb.com/bfs/album/d6d6ab3d8cedce02752e8c60c048579c5a2881d2.png)

上面的红色折线图就可以由四个sigmoid函数组成

![image-20221101144408416](https://i0.hdslb.com/bfs/album/af22bb8c3b683189a87dcd889263249bf61fec32.png)

然后将我们想要的内容整理一下，就可以将上图灰框内的数字整合成下面图中的样子

![image-20221101145512848](https://i0.hdslb.com/bfs/album/95150c134cd265258bca52f6f2ff5a05cb96dc3f.png)

![image-20221101145327017](https://i0.hdslb.com/bfs/album/6090f3f00856e990e928740918469361a0f4e3e6.png)

最后整理一下就可以得到结果

![image-20221101145714277](https://i0.hdslb.com/bfs/album/f28c298ca60307e3da959c4502a23770a59d93b1.png)

将所有的未知数放到一列

![image-20221101150055174](https://i0.hdslb.com/bfs/album/65b3baa35ece9d7a1a0216d2aea07c3b1dbb88c9.png)

当参数很少的时候，可以使用暴力的方法，试出所有的未知数

sigmoid的函数越多的时候可以越逼近原来的函数图像

当尝试了几次后，得到一个预估的y，其差值可以得到一个LOSS函数

![image-20221101150641877](https://i0.hdslb.com/bfs/album/387900e7bacf157fe2b5719cccc478152499ef1e.png)

未知数拉长作为一个未知量的时候，成为一个向量，可以找出θ的start

然后要对每一个位置的参数进行求微分，集合起来就是一个向量

这里的g是梯度gradient

![image-20221101164241467](https://i0.hdslb.com/bfs/album/00ac9666fd91aa418e06402fc45b3880f56d581e.png)

将θ0的向量减去学习率×微分值，得到新的θ1

而在实际的机器学习中，并不会拿一个大L来计算梯度，而是将其拆分成多个Batch，分多次计算损失

每次更新一次参数叫做update，对每个batch变化也称为update

把所有的batch都看过一遍，叫做一个Epoch，可以有多次update，取决于batchSize有多大

![image-20221101164539909](https://i0.hdslb.com/bfs/album/efb59148fc35bcad9eca3dd6b74492a959590160.png)

sigmoid比较难以计算，所以一个sigmoid可以看成是relu相加，第二个拐点后相加后位直线

relu是0与另一条直线的取大值得到的结果

![image-20221102091415750](https://i0.hdslb.com/bfs/album/22bd2bcb5002f2c47e4213ce3fba06a046ad1b58.png)

我们之前是只用了一层sigmoid来模拟原来的数据，我们可以将第一层的结果，放入下一个层数得到新的数据

其中的权重W‘和偏置b'是全新的未知数

![image-20221102092003552](https://i0.hdslb.com/bfs/album/5c4d2aba4e2211551e0957317199ebe1edb7a482.png)

布置多层后，可以降低损失函数的数值

![image-20221102105343994](https://i0.hdslb.com/bfs/album/d22eb4a680abce182fab3226b176d6afb689459b.png)

我们一般将这些sigmoid和relu称之为神经元，很多的neuron就是神经网络

![image-20221102105614766](https://i0.hdslb.com/bfs/album/74bd414a474892f66dec949fd64662baeeb975e2.png)

其中每一层neuron被称之为layer，多层的layer被称为深度学习

![image-20221102105752159](https://i0.hdslb.com/bfs/album/badb19da240c175de6d4c271adb52522025225cf.png)

过拟合overfitting指的是在增加层数的基础上，在已知的数据上获得更好的数据，而在未知的数据上得到更差的结果

![image-20221102110042413](https://i0.hdslb.com/bfs/album/719ebd885d6d2f6b7556d62b0eee091dbe2e8e47.png)